{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMz0E/MGyxSH3eAHbwegCzn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aju22/RoPE-PyTorch/blob/main/RoPE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Rotary Positional Embeddings (RoPE)**\n",
        "\n",
        "*Rotary Position Embedding (RoPE) is a technique used in transformer-based models to incorporate positional information into token representations. Unlike traditional positional encodings that rely on sine and cosine functions, RoPE utilizes rotation matrices to encode both absolute and relative positional information. This method was proposed as a way to enhance the effectiveness of positional embeddings in transformers.*"
      ],
      "metadata": {
        "id": "rOpRtQR3iQ3m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The Math**\n",
        "\n",
        "*Rotary encoding transforms pairs of features by rotating in the 2D plane. That is, it organizes the d features as d/2\n",
        "pairs. Each pair can be considered a coordinate in a 2D plane, and the encoding will rotate it by an angle depending on the position of the token.*\n",
        "\n",
        "$$\\text{Let } x_m^{(1)} \\text{ and } x_m^{(2)} \\text {be two features of the key or}\\\\\n",
        " \\text{query of any head at position m.}\\\\\n",
        " \\text{For simplicity assume x has only two features. Then the transformation is:}$$\n",
        "\n",
        "\n",
        "$$\\text{RoPE}(x_m^{(1)}, x_m^{(2)}, m) =\n",
        "\\begin{bmatrix}\n",
        "\\cos(m\\theta) & -\\sin(m\\theta) \\\\\n",
        "\\sin(m\\theta) & \\cos(m\\theta)\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "x_m^{(1)} \\\\\n",
        "x_m^{(2)}\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "x_m^{(1)}\\cos(m\\theta) - x_m^{(2)}\\sin(m\\theta) \\\\\n",
        "x_m^{(2)}\\cos(m\\theta) + x_m^{(1)}\\sin(m\\theta)\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "\n",
        "$$ \\Theta = \\theta_i = 10,000^{-\\frac{2(i-1)}{d}} , where ( i \\in [1, 2, ..., 2d] ) \\text{ for the ( 2d ) pairs of features.}$$"
      ],
      "metadata": {
        "id": "WyaC1bJYi5FH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The Intuition**\n",
        "\n",
        "We would like to find a positional encoding function ***f(x,l)*** for ***x*** and its position ***l*** such that, for two items ***q*** and ***k*** and at positions ***m*** and ***n***, the innner product between ***f(q,m)*** and ***f(k,n)*** is sensitive only to the values ***q*** and ***k*** and their relative position ***m - n***.\n",
        "\n",
        " A key piece of information is the geometric definition of the dot product between Euclidean vectors:\n",
        "\n",
        " $$q \\cdot k = |q| |k| \\cos \\theta$$\n",
        "\n",
        " The RoPE embedding achieves this:\n",
        "\n",
        " \\begin{align}\n",
        "\\mathrm{RoPE}(x, m) &= xe^{mi\\theta} \\\\\n",
        "\\langle \\mathrm{RoPE}(q_j, m), \\mathrm{RoPE}(k_j, n)\\rangle &= \\langle q_j e^{mi\\theta}, k_j e^{ni\\theta} \\rangle \\\\\n",
        "&= q_j k_j e^{mi\\theta} \\overline{e^{ni\\theta}} \\\\\n",
        "&= q_j k_j e^{(m - n)i\\theta} \\\\\n",
        "&= \\mathrm{RoPE}(q_j k_j, m - n)\n",
        "\\end{align}\n",
        "\n"
      ],
      "metadata": {
        "id": "jxmdbvMAkX0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch"
      ],
      "metadata": {
        "id": "TFBGxdQef_WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RotaryPositionalEmbeddings(nn.Module):\n",
        "\n",
        "  def __init__(self, d: int, base: int = 10_000):\n",
        "\n",
        "    super().__init__()\n",
        "    self.base = base\n",
        "    self.d = d\n",
        "    self.cos_cached = None\n",
        "    self.sin_cached = None\n",
        "\n",
        "  def _build_cache(self, x: torch.Tensor):\n",
        "\n",
        "    if self.cos_cached is not None and x.shape[0] <= self.cos_cached.shape[0]:\n",
        "      return\n",
        "\n",
        "    seq_len = x.shape[0]\n",
        "\n",
        "    theta = 1. / (self.base ** (torch.arange(0, self.d, 2).float() / self.d)).to(x.device) # THETA = 10,000^(-2*i/d) or 1/10,000^(2i/d)\n",
        "\n",
        "    seq_idx = torch.arange(seq_len, device=x.device).float().to(x.device) #Position Index -> [0,1,2...seq-1]\n",
        "\n",
        "    idx_theta = torch.einsum('n,d->nd', seq_idx, theta)  #Calculates m*(THETA) = [ [0, 0...], [THETA_1, THETA_2...THETA_d/2], ... [seq-1*(THETA_1), seq-1*(THETA_2)...] ]\n",
        "\n",
        "    idx_theta2 = torch.cat([idx_theta, idx_theta], dim=1) # [THETA_1, THETA_2...THETA_d/2] -> [THETA_1, THETA_2...THETA_d]\n",
        "\n",
        "\n",
        "    self.cos_cached = idx_theta2.cos()[:, None, None, :] #Cache [cosTHETA_1, cosTHETA_2...cosTHETA_d]\n",
        "    self.sin_cached = idx_theta2.sin()[:, None, None, :] #cache [sinTHETA_1, sinTHETA_2...sinTHETA_d]\n",
        "\n",
        "  def _neg_half(self, x: torch.Tensor):\n",
        "\n",
        "    d_2 = self.d // 2 #\n",
        "\n",
        "    return torch.cat([-x[:, :, :, d_2:], x[:, :, :, :d_2]], dim=-1) # [x_1, x_2,...x_d] -> [-x_d/2, ... -x_d, x_1, ... x_d/2]\n",
        "\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "\n",
        "    self._build_cache(x)\n",
        "\n",
        "    neg_half_x = self._neg_half(x)\n",
        "\n",
        "    x_rope = (x * self.cos_cached[:x.shape[0]]) + (neg_half_x * self.sin_cached[:x.shape[0]]) # [x_1*cosTHETA_1 - x_d/2*sinTHETA_d/2, ....]\n",
        "\n",
        "    return x_rope"
      ],
      "metadata": {
        "id": "XNeygwV2gEWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*We pair up the positive cosines and negative half sines to get the final embeddings as follows*:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "x_m^{(i)} \\\\\n",
        "x_m^{(i+d/2)}\n",
        "\\end{bmatrix}\n",
        "= \\begin{bmatrix}\n",
        "x_m^{(i)}\\cos(m\\theta_i) - x_m^{(i+d/2)}\\sin(m\\theta_i) \\\\\n",
        "x_m^{(i+d/2)}\\cos(m\\theta_i) + x_m^{(i)}\\sin(m\\theta_i)\n",
        "\\end{bmatrix}$$"
      ],
      "metadata": {
        "id": "BkZgSdWYhNeF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test"
      ],
      "metadata": {
        "id": "i8FN5Z2YV6Z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([[1, 2, 3, 4], [4, 5, 6, 7], [7, 8, 9, 10]], dtype=torch.float)\n",
        "x = x[:, None, None, :]"
      ],
      "metadata": {
        "id": "VCvQyvh_Txk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RotaryPositionalEmbeddings(4)(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zcI4DomV5va",
        "outputId": "c73b6c19-af73-4dec-d096-1cae45e84c21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[  1.0000,   2.0000,   3.0000,   4.0000]]],\n",
              "\n",
              "\n",
              "        [[[ -2.8876,   4.9298,   6.6077,   7.0496]]],\n",
              "\n",
              "\n",
              "        [[[-11.0967,   7.7984,   2.6198,  10.1580]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RObvHOezWAkb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}